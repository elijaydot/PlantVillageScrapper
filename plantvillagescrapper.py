# -*- coding: utf-8 -*-
"""PlantVillageScrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qs0zx11a68l5AJBaG6LnG3U_QnQVL4oY
"""

from bs4 import BeautifulSoup
import json
import requests # Import the requests library

def scrape_plant_data(plant_name, url):
    """
    Scrapes plant data (basic info, diseases, and pests) from a Plant Village URL.

    Args:
        plant_name: The name of the plant (e.g., "watermelon").
        url: The URL of the plant's info page on Plant Village.

    Returns:
        A dictionary containing the scraped data.   
    """
    try:
        # Fetch the HTML content from the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)
        html_content = response.text

        soup = BeautifulSoup(html_content, 'html.parser')

        plant_data = {
            'basic_info': {},
            'diseases': {},
            'pests': {}
        }

        # Scrape basic information
        scrape_basic_info(soup, plant_data)

        # Scrape diseases
        scrape_diseases(soup, plant_data)

        # Scrape pests
        scrape_pests(soup, plant_data)

        return plant_data

    except requests.exceptions.RequestException as e:
        print(f"Error fetching the URL: {e}")
        return None
    except Exception as e:
        print(f"An error occurred during scraping: {e}")
        return None


def scrape_basic_info(soup, data):
    # Description
    desc_section = soup.find('div', id='info-Description')
    if desc_section:
        # Get all paragraphs until next section
        desc_paragraphs = []
        next_elem = desc_section.find_next_sibling()
        while next_elem and not next_elem.get('id', '').startswith('info-'):
            if next_elem.name == 'p':
                desc_paragraphs.append(next_elem.get_text(strip=True))
            next_elem = next_elem.find_next_sibling()

        data['basic_info']['description'] = ' '.join(desc_paragraphs)

        # Images in description - Generalize: Find all image links within the description section or immediately following it.
        desc_images = []
        # Look within the description section first
        for a in desc_section.find_all('a', href=True):
             img = a.find('img')
             if img:
                 img_data = {
                     'url': img.get('src', ''),
                     'full_url': a.get('href', ''),
                     'caption': a.get('title', ''),
                     'description': a.get('data-description', '')
                 }
                 desc_images.append(img_data)

        # Also check the next few siblings for image containers if not found directly within
        next_elem = desc_section.find_next_sibling()
        while next_elem and not next_elem.get('id', '').startswith('info-') and len(desc_images) == 0: # Stop if new section or images found
             if next_elem.name == 'div':
                 for a in next_elem.find_all('a', href=True):
                     img = a.find('img')
                     if img:
                         img_data = {
                             'url': img.get('src', ''),
                             'full_url': a.get('href', ''),
                             'caption': a.get('title', ''),
                             'description': a.get('data-description', '')
                         }
                         desc_images.append(img_data)
             next_elem = next_elem.find_next_sibling()


        data['basic_info']['description_images'] = desc_images


    # Uses
    uses_section = soup.find('div', id='info-Uses')
    if uses_section:
        uses_content = []
        next_elem = uses_section.find_next_sibling()
        while next_elem and not next_elem.get('id', '').startswith('info-'):
            if next_elem.name == 'p':
                uses_content.append(next_elem.get_text(strip=True))
            next_elem = next_elem.find_next_sibling()

        data['basic_info']['uses'] = ' '.join(uses_content)

    # Propagation
    prop_section = soup.find('div', id='info-Propagation')
    if prop_section:
        propagation_data = {}

        # Get all content until references section
        content = []
        next_elem = prop_section.find_next_sibling()
        while next_elem and not next_elem.get('id', '').startswith('info-'):
            if next_elem.name in ['p', 'b']:
                content.append(next_elem.get_text(strip=True))
            next_elem = next_elem.find_next_sibling()


        # Join all content for now (could be parsed into subsections)
        propagation_data['content'] = ' '.join(content)

        # Propagation images - Generalize: Find all image links within the propagation section or immediately following it.
        prop_images = []
        # Look within the propagation section first
        for a in prop_section.find_all('a', href=True):
             img = a.find('img')
             if img:
                 img_data = {
                     'url': img.get('src', ''),
                     'full_url': a.get('href', ''),
                     'caption': a.get('title', ''),
                     'description': a.get('data-description', '')
                 }
                 prop_images.append(img_data)

        # Also check the next few siblings for image containers if not found directly within
        next_elem = prop_section.find_next_sibling()
        while next_elem and not next_elem.get('id', '').startswith('info-') and len(prop_images) == 0: # Stop if new section or images found
             if next_elem.name == 'div':
                 for a in next_elem.find_all('a', href=True):
                     img = a.find('img')
                     if img:
                         img_data = {
                             'url': img.get('src', ''),
                             'full_url': a.get('href', ''),
                             'caption': a.get('title', ''),
                             'description': a.get('data-description', '')
                         }
                         prop_images.append(img_data)
             next_elem = next_elem.find_next_sibling()

        propagation_data['images'] = prop_images


        data['basic_info']['propagation'] = propagation_data


    # References
    ref_section = soup.find('div', id='info-References')
    if ref_section:
        ref_content = []
        next_elem = ref_section.find_next_sibling('p')
        while next_elem and not next_elem.get('id', '').startswith('info-'):
            ref_content.append(next_elem.get_text(strip=True))
            next_elem = next_elem.find_next_sibling('p')


        data['basic_info']['references'] = ' '.join(ref_content)


def scrape_diseases(soup, data):
    diseases_div = soup.find('div', id='diseases')
    if not diseases_div:
        return

    disease_categories = [
        {'id': 'disease-Fungal', 'name': 'Fungal'},
        {'id': 'disease-Viral', 'name': 'Viral'},
        {'id': 'disease-Bacterial', 'name': 'Bacterial'},
        {'id': 'disease-Other', 'name': 'Other'},
        {'id': 'disease-Bacterial-Fungal', 'name': 'Bacterial, Fungal'}
    ]

    for category in disease_categories:
        category_div = soup.find('div', id=category['id'])
        if not category_div:
            continue

        diseases = []

        # Find all h4 elements that are siblings after the category div
        current = category_div.find_next_sibling()
        while current and not (current.name == 'div' and current.get('id', '').startswith('disease-')):
            if current.name == 'h4':
                disease = {
                    'name': current.get_text(strip=True),
                    'pathogen': None,
                    'symptoms': None,
                    'cause': None,
                    'comments': None,
                    'management': None,
                    'images': []
                }

                # Extract pathogen from span if exists
                pathogen_span = current.find('span', style="font-weight:400;font-size:80%;")
                if pathogen_span:
                    disease['pathogen'] = pathogen_span.get_text(strip=True)

                # Find symptoms, cause, comments, management, and images
                next_elem = current.find_next_sibling()
                while next_elem and not (next_elem.name == 'h4' or
                                       (next_elem.name == 'div' and next_elem.get('id', '').startswith('disease-'))):
                    if next_elem.name == 'div':
                        if 'symptoms' in next_elem.get('class', []):
                            disease['symptoms'] = next_elem.get_text(strip=True)
                        elif 'cause' in next_elem.get('class', []):
                            disease['cause'] = next_elem.get_text(strip=True)
                        elif 'comments' in next_elem.get('class', []):
                            disease['comments'] = next_elem.get_text(strip=True)
                        elif 'management' in next_elem.get('class', []):
                            disease['management'] = next_elem.get_text(strip=True)

                    # Generalize image finding: look for image links within the current div or the next sibling div
                    for a in next_elem.find_all('a', href=True):
                        img = a.find('img')
                        if img:
                             img_data = {
                                 'url': img.get('src', ''),
                                 'full_url': a.get('href', ''),
                                 'caption': a.get('title', ''),
                                 'description': a.get('data-description', '')
                             }
                             # Avoid duplicate images if they appear in multiple elements within the section
                             if img_data not in disease['images']:
                                disease['images'].append(img_data)


                    next_elem = next_elem.find_next_sibling()


                diseases.append(disease)

            current = current.find_next_sibling()

        data['diseases'][category['name']] = diseases

def scrape_pests(soup, data):
    pests_div = soup.find('div', id='pests')
    if not pests_div:
        return

    pest_categories = [
        {'id': 'pest-Insects', 'name': 'Insects'},
        {'id': 'pest-Mites', 'name': 'Mites'},
        {'id': 'pest-Nematodes', 'name': 'Nematodes'}
    ]

    for category in pest_categories:
        category_div = soup.find('div', id=category['id'])
        if not category_div:
            continue

        pests = []

        # Find all h4 elements that are siblings after the category div
        current = category_div.find_next_sibling()
        while current and not (current.name == 'div' and current.get('id', '').startswith('pest-')):
            if current.name == 'h4':
                pest = {
                    'name': current.get_text(strip=True),
                    'scientific_name': None,
                    'symptoms': None,
                    'cause': None,
                    'comments': None,
                    'management': None,
                    'images': []
                }

                # Extract scientific name from span if exists
                sci_name_span = current.find('span', style="font-weight:400;font-size:80%;")
                if sci_name_span:
                    pest['scientific_name'] = sci_name_span.get_text(strip=True)

                # Find symptoms, cause, comments, management, and images
                next_elem = current.find_next_sibling()
                while next_elem and not (next_elem.name == 'h4' or (next_elem.name == 'div' and next_elem.get('id', '').startswith('pest-'))):

                  if next_elem.name == 'div':
                    if 'symptoms' in next_elem.get('class', []):
                        pest['symptoms'] = next_elem.get_text(strip=True)
                    elif 'cause' in next_elem.get('class', []):
                        pest['cause'] = next_elem.get_text(strip=True) # Corrected syntax here
                    elif 'comments' in next_elem.get('class', []):
                        pest['comments'] = next_elem.get_text(strip=True)
                    elif 'management' in next_elem.get('class', []):
                        pest['management'] = next_elem.get_text(strip=True)

                  # Generalize image finding: look for image links within the current div or the next sibling div
                  for a in next_elem.find_all('a', href=True):
                      img = a.find('img')
                      if img:
                           img_data = {
                               'url': img.get('src', ''),
                               'full_url': a.get('href', ''),
                               'caption': a.get('title', ''),
                               'description': a.get('data-description', '')
                           }
                           # Avoid duplicate images if they appear in multiple elements within the section
                           if img_data not in pest['images']:
                              pest['images'].append(img_data)

                  next_elem = next_elem.find_next_sibling()


                pests.append(pest)

            current = current.find_next_sibling()

        data['pests'][category['name']] = pests

# Plant Scrapper usage
if __name__ == "__main__":
    # Get plant name from user input
    plant_to_scrape = input("Enter the name of the plant you want to scrape (e.g., watermelon): ").lower() # Convert to lowercase for URL consistency
    url_to_scrape = f'https://plantvillage.psu.edu/topics/{plant_to_scrape}/infos'

    plant_data = scrape_plant_data(plant_to_scrape, url_to_scrape)

    if plant_data:
        # Save to JSON file
        file_name = f'{plant_to_scrape}_data_web.json'
        with open(file_name, 'w', encoding='utf-8') as f:
            json.dump(plant_data, f, indent=2, ensure_ascii=False)

        print(f"{plant_to_scrape.capitalize()} data scraped from the web and saved to {file_name}")
    else:
        print(f"Failed to scrape data for {plant_to_scrape}.")